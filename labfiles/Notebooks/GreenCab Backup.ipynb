{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import org.apache.spark.sql.{Dataset, SparkSession}\nadls_path: String = abfss://nyctlc@labignitemodel.dfs.core.windows.net/green/"
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Set the path to read the Green Cab files\n",
        "import org.apache.spark.sql.{Dataset, SparkSession}\n",
        "spark.conf.set(\"fs.azure.account.auth.type.<ADLSg2 Account Name>.dfs.core.windows.net\", \"SharedKey\")\n",
        "spark.conf.set(\"fs.azure.account.key.<ADLSg2 Account Name>.dfs.core.windows.net\",\"<Primary Key>\")\n",
        "val adls_path = \"abfss://nyctlc@<ADLSg2 Account Name>.dfs.core.windows.net/green/\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "greencab: org.apache.spark.sql.DataFrame = [vendorID: int, lpepPickupDatetime: timestamp ... 23 more fields]"
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val greencab = spark.read.parquet(adls_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "greencabcurated: org.apache.spark.sql.DataFrame = [lpepPickupDatetime: timestamp, lpepDropoffDatetime: timestamp ... 11 more fields]"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Curate the dataframe with the years that you need and drop some columns that you don't need \n",
        "val greencabcurated = greencab.filter($\"puYear\" === \"2015\" || $\"puYear\" === \"2016\" || $\"puYear\" === \"2017\" || $\"puYear\" === \"2018\" || $\"puYear\" === \"2019\").drop(\"vendorID\",\"tripDistance\",\"rateCodeId\",\"storeAndFwdFlag\",\"paymentType\",\"fareAmount\",\"extra\",\"mtaTax\",\"improvementSurcharge\",\"tollsAmount\",\"ehailFee\",\"tripType\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "adls_pathsql: String = abfss://nyctlc@labignitemodel.dfs.core.windows.net/greencuratedbackup/"
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val adls_pathsql = \"abfss://nyctlc@<ADLSg2 Account Name>.dfs.core.windows.net/greencuratedbackup/\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "greencabcurated.write.mode(\"Overwrite\").parquet(adls_pathsql)"
      ],
      "attachments": {}
    }
  ]
}