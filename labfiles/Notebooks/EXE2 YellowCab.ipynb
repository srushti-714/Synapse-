{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import org.apache.spark.sql.{Dataset, SparkSession}\nadls_path: String = abfss://nyctlc@labignitemodel.dfs.core.windows.net/green/"
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Set the path to read the Yellow Cab files\n",
        "import org.apache.spark.sql.{Dataset, SparkSession}\n",
        "val adls_path = \"abfss://nyctlc@<ADLSg2 Account Name>.dfs.core.windows.net/yellow/\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error from http request: OK",
          "evalue": "",
          "traceback": [
            "Error from http request: OK",
            "Trace ID: 7220fe03-5d01-4be9-9776-71c432e5b207",
            "Something went wrong while processing your request. Please try again later."
          ]
        }
      ],
      "metadata": {},
      "source": [
        "// Read the yellow cab into a dataframe\n",
        "val yellowcab = spark.read.parquet(adls_path)\n",
        "yellowcab.show()\n",
        "yellowcab.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "yellowcabcurated: org.apache.spark.sql.DataFrame = [lpepPickupDatetime: timestamp, lpepDropoffDatetime: timestamp ... 11 more fields]"
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Curate the dataframe with the years that you need and drop some columns that you don't need \n",
        "val yellowcabcurated = yellowcab.filter($\"puYear\" === \"2018\").drop(\"vendorID\",\"tripDistance\",\"rateCodeId\",\"storeAndFwdFlag\",\"paymentType\",\"tpepPickupDateTime\",\"tpepDropoffDateTime\", \"fareAmount\",\"extra\",\"mtaTax\",\"improvementSurcharge\",\"tollsAmount\",\"ehailFee\",\"tripType\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- lpepPickupDatetime: timestamp (nullable = true)\n |-- lpepDropoffDatetime: timestamp (nullable = true)\n |-- passengerCount: integer (nullable = true)\n |-- puLocationId: string (nullable = true)\n |-- doLocationId: string (nullable = true)\n |-- pickupLongitude: double (nullable = true)\n |-- pickupLatitude: double (nullable = true)\n |-- dropoffLongitude: double (nullable = true)\n |-- dropoffLatitude: double (nullable = true)\n |-- tipAmount: double (nullable = true)\n |-- totalAmount: double (nullable = true)\n |-- puYear: integer (nullable = true)\n |-- puMonth: integer (nullable = true)"
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "yellowcabcurated.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "java.lang.NullPointerException",
          "traceback": [
            "Error : java.lang.NullPointerException",
            "  at java.util.Hashtable.put(Hashtable.java:460)\n",
            "  at java.util.Properties.setProperty(Properties.java:166)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$1.apply(JDBCOptions.scala:48)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$1.apply(JDBCOptions.scala:48)\n",
            "  at scala.collection.immutable.Map$Map4.foreach(Map.scala:188)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:48)\n",
            "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)\n",
            "  at com.microsoft.spark.azuredw.utils.AzureDWJDBCWrapper.createConnection(AzureDWJDBCWrapper.scala:67)\n",
            "  at com.microsoft.spark.azuredw.utils.Utils$.createConnection(Utils.scala:279)\n",
            "  at com.microsoft.spark.azuredw.write.AzureDWDataSourceWriter.createWriterFactory(AzureDWDataSourceWriter.scala:43)\n",
            "  at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:55)\n",
            "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
            "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
            "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
            "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
            "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
            "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
            "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
            "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
            "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
            "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
            "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
            "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
            "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
            "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:260)\n",
            "  at org.apache.spark.sql.AzureDWConnector$AzDWFormatWriter.azuredw(AzureDWConnector.scala:137)\n",
            "  ... 53 elided\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "yellowcabcurated.write.option(Constants.TEMP_FOLDER, \"abfss://tempdata@<ADLSg2 Account Name>.dfs.core.windows.net/\").\n",
        "    sqlanalytics(\"<SQL Pool Name>.staging.YellowCabNotebook\", Constants.INTERNAL)\n",
        ""
      ],
      "attachments": {}
    }
  ]
}