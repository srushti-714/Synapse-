{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import org.apache.spark.sql.{Dataset, SparkSession}\nadls_path: String = abfss://tempdata@labworkspace108242.dfs.core.windows.net/Marketsharepbiview_PBI.csv"
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Set the path to read the Yellow Cab files\n",
        "import org.apache.spark.sql.{Dataset, SparkSession}\n",
        "val adls_path = \"abfss://tempdata@labworkspace108242.dfs.core.windows.net/Marketsharepbiview_PBI.csv\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "df: org.apache.spark.sql.DataFrame = [PickupDate: timestamp, fhvrides: int ... 2 more fields]\n+-------------------+--------+-----------+----------+\n|         PickupDate|fhvrides|Yellowrides|Greenrides|\n+-------------------+--------+-----------+----------+\n|2016-12-26 00:00:00|  285508|     205192|     23392|\n|2017-06-30 00:00:00|  524810|     302847|     35505|\n|2016-10-21 00:00:00|  487689|     376369|     46276|\n|2015-06-11 00:00:00|  155609|     439869|     54240|\n|2015-02-10 00:00:00|   97965|     424564|     45711|\n|2016-01-06 00:00:00|  267690|     348516|     42025|\n|2018-01-05 00:00:00|  699753|     265212|     27459|\n|2017-11-06 00:00:00|  528282|     298503|     26212|\n|2017-05-21 00:00:00|  487727|     319407|     36892|\n|2016-12-09 00:00:00|  543698|     405547|     48307|\n|2018-04-18 00:00:00|  670809|     320981|     26088|\n|2018-01-08 00:00:00|  595390|     259792|     25476|\n|2015-11-27 00:00:00|  212675|     275078|     41880|\n|2015-07-27 00:00:00|  132925|     348629|     39821|\n|2017-04-28 00:00:00|  543456|     359350|     39685|\n|2016-02-18 00:00:00|  332941|     410641|     50612|\n|2017-10-11 00:00:00|  531008|     322317|     28714|\n|2015-01-02 00:00:00|   61832|     345296|     43410|\n|2015-01-30 00:00:00|  117671|     483380|     62944|\n|2018-03-06 00:00:00|  655452|     315726|     27427|\n+-------------------+--------+-----------+----------+\nonly showing top 20 rows\n\nroot\n |-- PickupDate: timestamp (nullable = true)\n |-- fhvrides: integer (nullable = true)\n |-- Yellowrides: integer (nullable = true)\n |-- Greenrides: integer (nullable = true)"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "val df = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(adls_path)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import com.microsoft.spark.sqlanalytics.utils.Constants\nimport org.apache.spark.sql.SqlAnalyticsConnector._"
          },
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "df.write.option(Constants.TEMP_FOLDER, \"abfss://tempdata@labworkspace108242.dfs.core.windows.net/\").sqlanalytics(\"sqlpool.dbo.Marketsharepbiview_PBI\", Constants.INTERNAL)\n",
        ""
      ],
      "attachments": {}
    }
  ]
}